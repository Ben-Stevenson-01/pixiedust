<!DOCTYPE html>
<html>
 <head>
  <title>
   Use Scala in a Python Notebook
  </title>
  <meta content="&#xA9;Copyright IBM Corporation 2017" name="copyright"/>
  <meta content="&#xA9;Copyright IBM Corporation 2017" name="DC.Rights.Owner"/>
  <meta content="&#xA9; Copyright IBM Corporation 2016, 2017" name="dcterms.rights"/>
  <meta content="2017-11-30" name="DC.date"/>
 </head>
 <body>
  <div>
   <h1>
    Use Scala in a Python Notebook
   </h1>
   <div>
    <h1>
     Introduction
    </h1>
    <p>
     Python has a rich ecosystem of modules including plotting with Matplotlib, data structure and analysis with Pandas, Machine Learning or Natural Language Processing. However, data scientists working with Spark may occasionaly need to call out one of the hundreds of libraries available on
     <a href="https://spark-packages.org/">
      spark-packages.org
     </a>
     which are written in Scala or Java. Unfortunately, Jupyter Python notebooks do not currently provide a way to call out scala code. As a result, a typical workaround is to first use a Scala notebook to run the Scala code, persist the output somewhere like a Hadoop Distributed File System, create another Python notebook, and re-load the data. This is obviously inefficent and awkward.
    </p>
    <p>
     PixieDust provides a solution to this problem by letting users directly write and run scala code in its own cell. It also lets variables be shared between Python and Scala and vice-versa.
    </p>
   </div>
   <div>
    <h1>
     Using Scala cell magic
    </h1>
    <p>
     After importing the PixieDust module, users can simply use the %%scala magic to write Scala code and run the cell as any other normal cell. Any compilation error will be reported in the cell output.
The following example shows how to run the
     <a href="https://developer.ibm.com/clouddataservices/sentiment-analysis-of-twitter-hashtags/">
      Watson sentiment sample app
     </a>
     written in Scala:
    </p>
    <p>
     First install the jar into the Python kernel:
    </p>
    <blockquote>
     <pre>import pixiedust
pixiedust.installPackage("https://github.com/ibm-cds-labs/spark.samples/raw/master/dist/streaming-twitter-assembly-1.6.jar")</pre>
    </blockquote>
    <p>
     You can now run the Scala code that uses Spark Streaming to fetch tweets:
    </p>
    <blockquote>
     <pre>%%scala
val demo = com.ibm.cds.spark.samples.StreamingTwitter
demo.setConfig("twitter4j.oauth.consumerKey","XXXX")
demo.setConfig("twitter4j.oauth.consumerSecret","XXXX")
demo.setConfig("twitter4j.oauth.accessToken","XXXX")
demo.setConfig("twitter4j.oauth.accessTokenSecret","XXXX")
demo.setConfig("watson.tone.url","https://gateway.watsonplatform.net/tone-analyzer/api")
demo.setConfig("watson.tone.password","XXXX")
demo.setConfig("watson.tone.username","XXXX")

import org.apache.spark.streaming._
demo.startTwitterStreaming(sc, Seconds(30))</pre>
    </blockquote>
   </div>
   <div>
    <h1>
     Variable transfer
    </h1>
    <p>
     Every variable defined within Python are accessible in Scala.
For example:
    </p>
    <blockquote>
     <pre>#define variables in python
var1="Hello"
var2=200</pre>
    </blockquote>
    <p>
     You can then access these variables in Scala
    </p>
    <blockquote>
     <pre>println(var1)
println(var2 + 10)</pre>
    </blockquote>
    <p>
     Likewise, you can transfer variables defined in Scala by prefixing them with __ (2 underscores). The following example shows how to access the tweets into a dataframe and transfer them into the python shell namespace:
    </p>
    <blockquote>
     <pre>%%scala
val demo = com.ibm.cds.spark.samples.StreamingTwitter
val (__sqlContext, __df) = demo.createTwitterDataFrames(sc)</pre>
    </blockquote>
    <p>
     Then use the __df variable in Python
    </p>
    <blockquote>
     <pre>print(__df.count())</pre>
    </blockquote>
   </div>
  </div>
 </body>
</html>
